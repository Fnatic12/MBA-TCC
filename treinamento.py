import glob
import os
import sys
import random
import time
import numpy as np
import math
import tensorflow as tf
from collections import deque
from tqdm import tqdm
from keras.applications import MobileNetV2
from keras.layers import Dense, GlobalAveragePooling2D
from keras.models import Model
from keras.optimizers import Adam

# üìå Configura√ß√£o do TensorFlow para GPU
gpus = tf.config.list_physical_devices('GPU')
if gpus:
    for gpu in gpus:
        tf.config.experimental.set_memory_growth(gpu, True)

# üìå Importando o CARLA
try:
    sys.path.append(glob.glob('../carla/dist/carla-*%d.%d-%s.egg' % (
        sys.version_info.major,
        sys.version_info.minor,
        'win-amd64' if os.name == 'nt' else 'linux-x86_64'))[0])
    import carla
    print("‚úÖ M√≥dulo CARLA importado com sucesso!")
except IndexError:
    print("üö´ ERRO: Falha ao importar o m√≥dulo CARLA. Verifique o caminho do .egg.")
    sys.exit(1)

# üîπ Configura√ß√µes do ambiente
IM_WIDTH = 640
IM_HEIGHT = 480
REPLAY_MEMORY_SIZE = 5000
MIN_REPLAY_MEMORY_SIZE = 1000
MINIBATCH_SIZE = 32
DISCOUNT = 0.99
EPSILON = 1.0
EPSILON_DECAY = 0.995
MIN_EPSILON = 0.01
EPISODES = 1000
FPS = 60
MODEL_NAME = "MobileNet_DQN"

# üîπ Criando diret√≥rio para logs do TensorBoard
LOG_DIR = "logs_treinamento"
os.makedirs(LOG_DIR, exist_ok=True)
writer = tf.summary.create_file_writer(LOG_DIR)

# ‚úÖ Ambiente CARLA
class CarEnv:
    def __init__(self):
        self.client = carla.Client("localhost", 2000)
        self.client.set_timeout(10.0)
        self.world = self.client.get_world()
        self.blueprint_library = self.world.get_blueprint_library()
        self.model_3 = self.blueprint_library.filter("model3")[0]
        self.actor_list = []
        self.front_camera = None
        self.collision_hist = []

    def reset(self):
        """Reseta o ambiente e inicia um novo epis√≥dio"""
        self.clear_actors()
        self.collision_hist = []
        self.total_distance = 0  # üîπ Resetando dist√¢ncia percorrida

        spawn_point = random.choice(self.world.get_map().get_spawn_points())
        self.vehicle = self.world.spawn_actor(self.model_3, spawn_point)
        self.actor_list.append(self.vehicle)

        # üîπ Sensor de c√¢mera
        self.rgb_cam = self.blueprint_library.find('sensor.camera.rgb')
        self.rgb_cam.set_attribute("image_size_x", f"{IM_WIDTH}")
        self.rgb_cam.set_attribute("image_size_y", f"{IM_HEIGHT}")
        self.rgb_cam.set_attribute("fov", "110")

        transform = carla.Transform(carla.Location(x=2.5, z=0.7))
        self.sensor = self.world.spawn_actor(self.rgb_cam, transform, attach_to=self.vehicle)
        self.actor_list.append(self.sensor)
        self.sensor.listen(lambda data: self.process_img(data))

        # üîπ Sensor de colis√£o
        colsensor = self.blueprint_library.find("sensor.other.collision")
        self.colsensor = self.world.spawn_actor(colsensor, transform, attach_to=self.vehicle)
        self.actor_list.append(self.colsensor)
        self.colsensor.listen(lambda event: self.collision_data(event))

        while self.front_camera is None:
            time.sleep(0.01)

        return self.front_camera

    def process_img(self, image):
        """Processa a imagem da c√¢mera"""
        i = np.array(image.raw_data)
        i2 = i.reshape((IM_HEIGHT, IM_WIDTH, 4))
        self.front_camera = i2[:, :, :3]

    def collision_data(self, event):
        """Detecta colis√µes"""
        self.collision_hist.append(event)

    def step(self, action):
        """Executa a√ß√£o e retorna observa√ß√µes"""
        controls = [
            carla.VehicleControl(throttle=0.6, steer=-0.5),
            carla.VehicleControl(throttle=0.6, steer=0.0),
            carla.VehicleControl(throttle=0.6, steer=0.5),
            carla.VehicleControl(throttle=0.2, steer=0.0),
            carla.VehicleControl(throttle=0.0, brake=1.0)
        ]
        self.vehicle.apply_control(controls[action])

        time.sleep(1 / FPS)  # üîπ Ajuste de tempo para manter FPS constante

        v = self.vehicle.get_velocity()
        kmh = 3.6 * math.sqrt(v.x**2 + v.y**2 + v.z**2)  # Velocidade em km/h

        self.total_distance += (kmh * 1000 / 3600) * (1 / FPS)  # üîπ Convertendo para metros e acumulando

        reward = 10 if kmh > 30 else -5
        if len(self.collision_hist) != 0:
            reward = -200
            done = True
        else:
            done = False

        return self.front_camera, reward, done, kmh

    def clear_actors(self):
        """Remove sensores e ve√≠culos anteriores"""
        for actor in self.actor_list:
            actor.destroy()
        self.actor_list = []

# ‚úÖ Modelo DQN
class DQNAgent:
    def __init__(self):
        self.model = self.create_model()
        self.target_model = self.create_model()
        self.target_model.set_weights(self.model.get_weights())

        self.replay_memory = deque(maxlen=REPLAY_MEMORY_SIZE)
        self.target_update_counter = 0

    def create_model(self):
        base_model = MobileNetV2(weights=None, include_top=False, input_shape=(IM_HEIGHT, IM_WIDTH, 3))
        x = GlobalAveragePooling2D()(base_model.output)
        predictions = Dense(5, activation="linear")(x)
        model = Model(inputs=base_model.input, outputs=predictions)
        model.compile(loss="mse", optimizer=Adam(learning_rate=0.001))
        return model

# ‚úÖ Execu√ß√£o do treinamento
if __name__ == '__main__':
    env = CarEnv()
    agent = DQNAgent()

    for episode in tqdm(range(1, EPISODES + 1), ascii=True, unit='epis√≥dios'):
        env.collision_hist = []
        state = env.reset()
        done = False
        total_reward = 0
        steps = 0
        total_velocity = 0

        while not done:
            if np.random.random() > EPSILON:
                action = np.argmax(agent.model.predict(np.expand_dims(state, axis=0) / 255.0))
            else:
                action = np.random.randint(0, 5)

            new_state, reward, done, kmh = env.step(action)
            total_reward += reward
            total_velocity += kmh
            steps += 1
            state = new_state

        EPSILON = max(MIN_EPSILON, EPSILON * EPSILON_DECAY)

        # üîπ C√°lculo da velocidade m√©dia e dist√¢ncia percorrida
        avg_velocity = total_velocity / max(1, steps)  # Evita divis√£o por zero
        distance_covered = env.total_distance

        # üîπ Salvando m√©tricas
        with writer.as_default():
            tf.summary.scalar("Recompensa Total", total_reward, step=episode)
            tf.summary.scalar("Passos no Epis√≥dio", steps, step=episode)
            tf.summary.scalar("Velocidade M√©dia (km/h)", avg_velocity, step=episode)
            tf.summary.scalar("Dist√¢ncia Percorrida (m)", distance_covered, step=episode)
            tf.summary.scalar("Epsilon", EPSILON, step=episode)
            writer.flush()

    agent.model.save("models/MobileNet_DQN.h5")
    print("‚úÖ Modelo salvo com sucesso!")